{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerbt/MultiDiffusion/blob/master/MultiDiffusion_Panorama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWT5Q9c_qJ6v"
      },
      "source": [
        "Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IIbLF70Rp-Dt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: diffusers in /home/codespace/.local/lib/python3.10/site-packages (0.20.1)\n",
            "Requirement already satisfied: importlib-metadata in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (6.8.0)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (0.17.2)\n",
            "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (2023.8.8)\n",
            "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (0.3.3)\n",
            "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (10.0.0)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers) (2023.9.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/codespace/.local/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->diffusers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->diffusers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests->diffusers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->diffusers) (2023.7.22)\n",
            "Requirement already satisfied: transformers in /home/codespace/.local/lib/python3.10/site-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
            "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cXXNWiTAqWNS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading (…)cheduler_config.json: 100%|██████████| 308/308 [00:00<00:00, 1.61MB/s]\n",
            "Downloading (…)ain/model_index.json: 100%|██████████| 537/537 [00:00<00:00, 3.11MB/s]\n",
            "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]\n",
            "Downloading (…)_encoder/config.json: 100%|██████████| 738/738 [00:00<00:00, 2.88MB/s]\n",
            "\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 460/460 [00:00<00:00, 1.80MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)f47/unet/config.json: 100%|██████████| 1.01k/1.01k [00:00<00:00, 5.20MB/s]\n",
            "\n",
            "\u001b[A\n",
            "\n",
            "Downloading (…)rocessor_config.json: 100%|██████████| 342/342 [00:00<00:00, 931kB/s]\n",
            "Downloading (…)tokenizer/vocab.json: 100%|██████████| 1.06M/1.06M [00:00<00:00, 21.8MB/s]\n",
            "\n",
            "\u001b[A\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 929/929 [00:00<00:00, 3.59MB/s]\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Downloading (…)tokenizer/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 2.55MB/s]\n",
            "\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)6f47/vae/config.json: 100%|██████████| 716/716 [00:00<00:00, 2.81MB/s]\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "Downloading (…)ch_model.safetensors: 100%|██████████| 335M/335M [00:04<00:00, 78.3MB/s]\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors: 100%|██████████| 1.36G/1.36G [00:10<00:00, 127MB/s]\n",
            "Fetching 12 files:  33%|███▎      | 4/12 [00:11<00:23,  2.95s/it]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)ch_model.safetensors: 100%|██████████| 3.46G/3.46G [00:18<00:00, 185MB/s]\n",
            "Fetching 12 files: 100%|██████████| 12/12 [00:19<00:00,  1.62s/it]\n",
            "Loading pipeline components...: 100%|██████████| 6/6 [01:02<00:00, 10.42s/it]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m scheduler \u001b[39m=\u001b[39m DDIMScheduler\u001b[39m.\u001b[39mfrom_pretrained(model_ckpt, subfolder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mscheduler\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pipe \u001b[39m=\u001b[39m StableDiffusionPanoramaPipeline\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m      model_ckpt, scheduler\u001b[39m=\u001b[39mscheduler, torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:727\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, torch_device, torch_dtype, silence_dtype_warnings)\u001b[0m\n\u001b[1;32m    723\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    724\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe module \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been loaded in 8bit and moving it to \u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m     )\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     module\u001b[39m.\u001b[39;49mto(torch_device, torch_dtype)\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    730\u001b[0m     module\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16\n\u001b[1;32m    731\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mstr\u001b[39m(torch_device) \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    732\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    733\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    734\u001b[0m ):\n\u001b[1;32m    735\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPipelines loaded with `torch_dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    741\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2065\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2061\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2062\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2063\u001b[0m     )\n\u001b[1;32m   2064\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2065\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPanoramaPipeline, DDIMScheduler\n",
        "\n",
        "model_ckpt = \"stabilityai/stable-diffusion-2-base\"\n",
        "scheduler = DDIMScheduler.from_pretrained(model_ckpt, subfolder=\"scheduler\")\n",
        "pipe = StableDiffusionPanoramaPipeline.from_pretrained(\n",
        "     model_ckpt, scheduler=scheduler, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StableDiffusionPanoramaPipeline {\n",
              "  \"_class_name\": \"StableDiffusionPanoramaPipeline\",\n",
              "  \"_diffusers_version\": \"0.20.1\",\n",
              "  \"_name_or_path\": \"stabilityai/stable-diffusion-2-base\",\n",
              "  \"feature_extractor\": [\n",
              "    \"transformers\",\n",
              "    \"CLIPImageProcessor\"\n",
              "  ],\n",
              "  \"requires_safety_checker\": false,\n",
              "  \"safety_checker\": [\n",
              "    null,\n",
              "    null\n",
              "  ],\n",
              "  \"scheduler\": [\n",
              "    \"diffusers\",\n",
              "    \"DDIMScheduler\"\n",
              "  ],\n",
              "  \"text_encoder\": [\n",
              "    \"transformers\",\n",
              "    \"CLIPTextModel\"\n",
              "  ],\n",
              "  \"tokenizer\": [\n",
              "    \"transformers\",\n",
              "    \"CLIPTokenizer\"\n",
              "  ],\n",
              "  \"unet\": [\n",
              "    \"diffusers\",\n",
              "    \"UNet2DConditionModel\"\n",
              "  ],\n",
              "  \"vae\": [\n",
              "    \"diffusers\",\n",
              "    \"AutoencoderKL\"\n",
              "  ]\n",
              "}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CZuMAP5sh0l"
      },
      "source": [
        "Create a Panorama!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXkdF7HOs77b"
      },
      "source": [
        "For more details, please visit our [project page](https://multidiffusion.github.io/) and our [paper](https://arxiv.org/abs/2302.08113)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "rjwy0QZHsflu",
        "outputId": "5e82b3ff-1753-43f1-fb36-521be16a3f4d"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "\"LayerNormKernelImpl\" not implemented for 'Half'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m W \u001b[39m=\u001b[39m \u001b[39m2048\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma photo of the dolomites\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m pipe(prompt, height\u001b[39m=\u001b[39;49mH, width\u001b[39m=\u001b[39;49mW)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bscaling-bassoon-x7vvrjw4r6426vqq/workspaces/MultiDiffusion/MultiDiffusion_Panorama.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m image\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py:569\u001b[0m, in \u001b[0;36mStableDiffusionPanoramaPipeline.__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, guidance_scale, view_batch_size, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, circular_padding)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39m# 3. Encode input prompt\u001b[39;00m\n\u001b[1;32m    566\u001b[0m text_encoder_lora_scale \u001b[39m=\u001b[39m (\n\u001b[1;32m    567\u001b[0m     cross_attention_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m cross_attention_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m )\n\u001b[0;32m--> 569\u001b[0m prompt_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_prompt(\n\u001b[1;32m    570\u001b[0m     prompt,\n\u001b[1;32m    571\u001b[0m     device,\n\u001b[1;32m    572\u001b[0m     num_images_per_prompt,\n\u001b[1;32m    573\u001b[0m     do_classifier_free_guidance,\n\u001b[1;32m    574\u001b[0m     negative_prompt,\n\u001b[1;32m    575\u001b[0m     prompt_embeds\u001b[39m=\u001b[39;49mprompt_embeds,\n\u001b[1;32m    576\u001b[0m     negative_prompt_embeds\u001b[39m=\u001b[39;49mnegative_prompt_embeds,\n\u001b[1;32m    577\u001b[0m     lora_scale\u001b[39m=\u001b[39;49mtext_encoder_lora_scale,\n\u001b[1;32m    578\u001b[0m )\n\u001b[1;32m    580\u001b[0m \u001b[39m# 4. Prepare timesteps\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mset_timesteps(num_inference_steps, device\u001b[39m=\u001b[39mdevice)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py:221\u001b[0m, in \u001b[0;36mStableDiffusionPanoramaPipeline._encode_prompt\u001b[0;34m(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds, negative_prompt_embeds, lora_scale)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m         attention_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     prompt_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_encoder(\n\u001b[1;32m    222\u001b[0m         text_input_ids\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m    223\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    225\u001b[0m     prompt_embeds \u001b[39m=\u001b[39m prompt_embeds[\u001b[39m0\u001b[39m]\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:840\u001b[0m, in \u001b[0;36mCLIPTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[39m>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 840\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_model(\n\u001b[1;32m    841\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    842\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    843\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    844\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    845\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    846\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    847\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:745\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m     \u001b[39m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     attention_mask \u001b[39m=\u001b[39m _expand_mask(attention_mask, hidden_states\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 745\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    746\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    747\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    748\u001b[0m     causal_attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    749\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    750\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    751\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    752\u001b[0m )\n\u001b[1;32m    754\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    755\u001b[0m last_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm(last_hidden_state)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:656\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    649\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    650\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    651\u001b[0m         hidden_states,\n\u001b[1;32m    652\u001b[0m         attention_mask,\n\u001b[1;32m    653\u001b[0m         causal_attention_mask,\n\u001b[1;32m    654\u001b[0m     )\n\u001b[1;32m    655\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 656\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    657\u001b[0m         hidden_states,\n\u001b[1;32m    658\u001b[0m         attention_mask,\n\u001b[1;32m    659\u001b[0m         causal_attention_mask,\n\u001b[1;32m    660\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    661\u001b[0m     )\n\u001b[1;32m    663\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    665\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:385\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[39m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39m        returned tensors for more detail.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 385\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_norm1(hidden_states)\n\u001b[1;32m    386\u001b[0m hidden_states, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    387\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    388\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    389\u001b[0m     causal_attention_mask\u001b[39m=\u001b[39mcausal_attention_mask,\n\u001b[1;32m    390\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    392\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"LayerNormKernelImpl\" not implemented for 'Half'"
          ]
        }
      ],
      "source": [
        "H = 512\n",
        "W = 2048\n",
        "\n",
        "prompt = \"a photo of the dolomites\"\n",
        "image = pipe(prompt, height=H, width=W).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b741-dbksxYB"
      },
      "source": [
        "Save the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2O10IhwBqdwS"
      },
      "outputs": [],
      "source": [
        "image.save(\"output.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNPvWMdSaVeHAmL7Ye6f0jg",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
